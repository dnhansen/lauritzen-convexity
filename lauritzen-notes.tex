% Document setup
\documentclass[article, a4paper, 11pt, oneside]{memoir}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[UKenglish]{babel}

% Document info
\newcommand\doctitle{Lauritzen: \emph{Undergraduate Convexity}}
\newcommand\docauthor{Danny NygÃ¥rd Hansen}

% Formatting and layout
\usepackage[autostyle]{csquotes}
\renewcommand{\mktextelp}{(\textellipsis\unkern)}
\usepackage[final]{microtype}
\usepackage{xcolor}
\frenchspacing
\usepackage{latex-sty/articlepagestyle}
\usepackage{latex-sty/articlesectionstyle}

% Fonts
\usepackage{amssymb}
\usepackage[largesmallcaps,partialup]{kpfonts}
\DeclareSymbolFontAlphabet{\mathrm}{operators} % https://tex.stackexchange.com/questions/40874/kpfonts-siunitx-and-math-alphabets
\linespread{1.06}
% \let\mathfrak\undefined
% \usepackage{eufrak}
\DeclareMathAlphabet\mathfrak{U}{euf}{m}{n}
\SetMathAlphabet\mathfrak{bold}{U}{euf}{b}{n}
% https://tex.stackexchange.com/questions/13815/kpfonts-with-eufrak
\usepackage{inconsolata}

% Hyperlinks
\usepackage{hyperref}
\definecolor{linkcolor}{HTML}{4f4fa3}
\hypersetup{%
	pdftitle=\doctitle,
	pdfauthor=\docauthor,
	colorlinks,
	linkcolor=linkcolor,
	citecolor=linkcolor,
	urlcolor=linkcolor,
	bookmarksnumbered=true
}

% Equation numbering
\numberwithin{equation}{chapter}

% Footnotes
\footmarkstyle{\textsuperscript{#1}\hspace{0.25em}}

% Mathematics
\usepackage{latex-sty/basicmathcommands}
\usepackage{latex-sty/framedtheorems}
\usepackage{latex-sty/topologycommands}
\usepackage{tikz-cd}
\tikzcdset{arrow style=math font} % https://tex.stackexchange.com/questions/300352/equalities-look-broken-with-tikz-cd-and-math-font
\usetikzlibrary{babel}

% Lists
\usepackage{enumitem}
\setenumerate[0]{label=\normalfont(\alph*)}
\setlist{
    listparindent=\parindent,
    parsep=0pt,
}

% Bibliography
\usepackage[backend=biber, style=authoryear, maxcitenames=2, useprefix]{biblatex}
\addbibresource{references.bib}

% Title
\title{\doctitle}
\author{\docauthor}

\newcommand{\setF}{\mathbb{F}}
\newcommand{\ev}{\mathrm{ev}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calU}{\mathcal{U}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calR}{\mathcal{R}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calK}{\mathcal{K}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\borel}{\mathcal{B}}
\newcommand{\measurable}{\mathcal{M}}
\newcommand{\wto}{\Rightarrow}
\DeclarePairedDelimiter{\net}{\langle}{\rangle}
\newcommand{\strucS}{\mathfrak{S}}
\DeclarePairedDelimiter{\gen}{\langle}{\rangle} % Generating set
\newcommand{\frakL}{\mathfrak{L}}

\newcommand{\bbK}{\mathbb{K}}

\DeclareMathOperator{\Span}{span}
\DeclareMathOperator*{\slim}{s-lim}

\newenvironment{displaytheorem}{%
	\begin{displayquote}\itshape%
}{%
	\end{displayquote}%
}


%% Framed exercise environment

\mdfdefinestyle{swannexercise}{%
    skipabove=0.5em plus 0.4em minus 0.2em,
	skipbelow=0.5em plus 0.4em minus 0.2em,
	leftmargin=-5pt,
	rightmargin=-5pt,
	innerleftmargin=5pt,
	innerrightmargin=5pt,
	innertopmargin=5pt,
	innerbottommargin=4pt,
	linewidth=0pt,
	splittopskip=1.2em minus 0.2em,
	splitbottomskip=0.5em plus 0.2em minus 0.1em,
	backgroundcolor=backgroundcolor,
	frametitlebackgroundcolor=titlecolor,
	frametitlefont={\scshape},
    theoremseparator={\space\thechapter},
    theoremspace={.},
	frametitleaboveskip=3pt,
	frametitlebelowskip=2pt
}

\mdtheorem[style=swannexercise]{exerciseframed}{Exercise}

\let\oldexerciseframed\exerciseframed
\renewcommand{\exerciseframed}{%
  \crefalias{theorem}{exerciseframed}%
  \oldexerciseframed}

\makeatother



\theoremstyle{nonumberplain}
\theoremsymbol{\ensuremath{\square}}
\newtheorem{solution}{Solution}

\let\oldsolution\solution
\renewcommand{\solution}{%
  \crefalias{theorem}{solution}%
  \oldsolution}

\newcommand{\solutionlabelfont}[1]{{\normalfont\color{linkcolor}#1}}
\newlist{solutionsec}{enumerate}{1}
\setlist[solutionsec]{leftmargin=0pt, parsep=0pt, listparindent=\parindent, font=\solutionlabelfont, label=(\alph*), labelsep=0pt, labelwidth=20pt, itemindent=20pt, align=left, itemsep=10pt}


\newcommand{\aff}{\operatorname{aff}}


\begin{document}

\maketitle

\addtocounter{chapter}{1}

\chapter{Affine subspaces}

\section*{General notes}

\begin{remark}
    Let $f \colon \reals^d \to \reals^m$ be a map. Then the following properties are equivalent:
    %
    \begin{enumerate}
        \item There exists a matrix $A$ and a vector $b$ such that $f(x) = Ax + b$ for all $x \in \reals^d$.
        
        \item For all $x,y \in \reals^d$ and $t \in \reals$, $f((1-t)x + ty) = (1-t)f(x) + tf(y)$.

        \item For all $x_1, \ldots, x_k \in \reals^d$ and $t_1, \ldots, t_k \in \reals$ with $t_1 + \cdots + t_k = 1$, $f(t_1 x_1 + \cdots + t_k x_k) = t_1 f(x_1) + \cdots + t_k f(x_k)$.
    \end{enumerate}
    %
    Such a function is called \emph{affine}. Note that if $m = 1$, then $f$ is affine if and only if it is both convex and concave.
    
    The first property clearly entails the third, which then entails the second, so assume that $f$ has the second property. First assume that $f(0) = 0$. For $\beta \in \reals$ and $x \in \reals^d$ be thus have
    %
    \begin{equation*}
        f(\beta x)
            = f(\beta x + (1-\beta)0)
            = \beta f(x) + (1-\beta)f(0)
            = \beta f(x),
    \end{equation*}
    %
    so $f$ is homogeneous. If also $y \in \reals^d$, then
    %
    \begin{equation*}
        \tfrac{1}{2} f(x + y)
            = f \bigl( \tfrac{1}{2} x + \tfrac{1}{2} y \bigr)
            = \tfrac{1}{2} f(x) + \tfrac{1}{2} f(y),
    \end{equation*}
    %
    so $f$ is also additive, hence linear. For general $f$, simply replace $f$ with $f - f(0)$.
\end{remark}


\begin{remark}
    Let $M \subseteq \reals^d$ be nonempty. Then the following properties are equivalent:
    %
    \begin{enumerate}
        \item For all $x,y \in M$ and $t \in \reals$, $(1-t)x + ty \in M$.

        \item For all $x_1, \ldots, x_k \in \reals^d$ and $t_1, \ldots, t_k \in \reals$ with $t_1 + \cdots + t_k = 1$, $t_1 x_1 + \cdots + t_k x_k \in M$.

        \item There is a linear subspace $W \subseteq \reals^d$ and an element $x_0 \in M$ such that $M = x_0 + W$. In this case $W = \set{x-y}{x,y \in M}$, and the identity $M = x_0 + W$ holds for all $x_0 \in M$.

        \item There is an $m \times d$ matrix $A$ and a vector $b$ such that $M = \set{x \in \reals^d}{Ax=b}$.

        \item There is an affine map $f \colon \reals^d \to \reals^m$ such that $M = \set{x \in \reals^d}{f(x) = 0}$.
    \end{enumerate}
    %
    If $M$ has any of these properties, then $M$ is called \emph{affine}.
    
    Clearly (b) implies (a), and the converse follows by noticing that when $k > 1$ at least one $t_1$ must be different from $1$, say $t_k$, so
    %
    \begin{equation*}
        t_1 x_1 + \cdots + t_k x_k
            = (1-t_k) \biggl( \frac{t_1}{1-t_k} x_1 + \cdots + \frac{t_{k-1}}{1-t_k} x_{k-1} \biggr) + t_k x_k.
    \end{equation*}
    %
    Hence (b) follows by induction. Assuming (b), let $W = \set{x-y}{x,y \in M}$, and consider $x_1,x_2,y_1,y_2 \in M$ and $\beta \in \reals$. Then
    %
    \begin{equation*}
        \beta(x_1-y_1) + (x_2-y_2)
            = (\beta x_1 + x_2 + (-\beta)y_1) - y_2
            \in W,
    \end{equation*}
    %
    so $W$ is a subspace. For any $x_0,x,y \in M$ and we have $x_0 + (x-y) \in M$, and conversely $x = x_0 + (x-x_0) \in x_0 + W$, so $M = x_0 + W$. If $W'$ is another subspace such that $M = x_0 + W'$ for some $x_0$, then $W' = M - x_0 = W$.
    
    If instead (c) holds, and $x,y \in M$ and $t \in \reals$, then $x = x_0 + x'$ and $y = x_0 + y'$ for some $x',y' \in W$, and so
    %
    \begin{equation*}
        (1-t)x + ty
            = x_0 + (1-t)x' + ty'
            \in x_0 + W,
    \end{equation*}
    %
    implying (a).

    The equivalence between (c), (d) and (e) is obvious since $W$ is the nullspace of some matrix, and we may choose any $x_0$ such that $Ax_0 = b$. (Furthermore, (e) easily implies (b).)
\end{remark}


\begin{remark}
    Let $f \colon \reals^d \to \reals^m$ be affine, and let $M \subseteq \reals^d$ be affine. Then the image $f[M]$ is also affine. Furthermore, for any $S \subseteq \reals^d$ we have $f[\aff S] = \aff f[S]$ (just as linear maps commute with span).

    If $f(x) = Ax + b$ and $M = x_0 + W$ is affine with $W$ a subspace, then
    %
    \begin{equation*}
        f[M]
            = f[x_0 + W]
            = A[x_0 + W] + b
            = (Ax_0 + b) + A[W].
    \end{equation*}
    %
    In particular, $\dim f[M] = \dim A[W]$. If $f$ (and hence $A$) is invertible, then we thus have $\dim f[M] = \dim W = \dim M$.
\end{remark}


\begin{remark}
    If $S \subseteq \reals^d$ and $x_0 \in S$, then we claim that $\aff S = x_0 + \Span(S - x_0)$. Consider the affine map $f(x) = x - x_0$. From the above remark we have
    %
    \begin{equation*}
        x_0 + \aff f[S]
            = x_0 + f[\aff S]
            = \aff S.
    \end{equation*}
    %
    But $f[S]$ contains $0$, so $\aff f[S]$ is a subspace and hence equal to $\Span f[S]$. Furthermore, since $f$ is invertible it preserves dimensions, so if $\card{S} = m$ and is positive and finite, then $\dim S = m-1$ if and only if $\dim W = m-1$, where $W = \Span(S - x_0)$. But $W$ is also spanned by $S \setminus \{x_0\} - x_0$ which has $m-1$ elements, so $S$ is affinely independent if and only if $S \setminus \{x_0\} - x_0$ is linearly independent.
\end{remark}


\begin{remark}
    If $f \colon \reals^d \to \reals^d$ preserves the Euclidean metric, then it is affine. Replacing $f$ with $f - f(0)$ we may assume that $f(0) = 0$, and so that $f$ also preserves the inner product. For all $x,y,z \in \reals^d$ and $\beta \in \reals$ we have
    %
    \begin{align*}
        \inner{f(\beta x + y)}{f(z)}
            &= \inner{\beta x + y}{z} \\
            &= \beta \inner{x}{z} + \inner{y}{z} \\
            &= \beta \inner{f(x)}{f(z)} + \inner{f(y)}{f(z)} \\
            &= \inner{\beta f(x) + f(y)}{f(z)}.
    \end{align*}
    %
    Since $f$ is surjective, the claim follows.
\end{remark}


\begin{remarkbreak}[The affine hull and dimension of the empty set]
    By definition, the affine hull of the empty set is the empty set. This is the case by Definition~2.4, and this is also the standard definition. Hence the affine hull of the empty set is not an affine subspace! In particular, it cannot have a dimension in the sense of Definition~2.7.
\end{remarkbreak}


\section*{Exercises}

% [TODO exercise references]

\begin{exerciseframed*}[10]
    Prove that you can have no more than $d+1$ affinely independent vectors in $\reals^d$.
\end{exerciseframed*}

\begin{solution}
    Let $\{v_1, \ldots, v_n\} \subseteq \reals^d$ be affinely independent. By Proposition~2.9, the $n-1$ vectors $v_2 - v_1, \ldots, v_n - v_1$ vectors are linearly independent. But then we must have $n-1 \leq d$, i.e. $n \leq d+1$.
\end{solution}


\begin{exerciseframed*}[11]
    Let $v_0, \ldots, v_d$ be affinely independent points in $\reals^d$. Prove that
    %
    \begin{equation*}
        f(x) = (\lambda_0, \lambda_1, \ldots, \lambda_d)
    \end{equation*}
    %
    is a well defined affine map $f \colon \reals^d \to \reals^{d+1}$, where
    %
    \begin{equation*}
        x = \lambda_0 v_0 + \cdots + \lambda_d v_d
    \end{equation*}
    %
    with $\lambda_0 + \cdots + \lambda_d = 1$.
\end{exerciseframed*}

\begin{solution}
    If also $x = \mu_0 v_0 + \cdots + \mu_d v_d$ with $\mu_0 + \cdots + \mu_d = 1$, then
    %
    \begin{equation*}
        (\lambda_0 - \mu_0) v_0 + \cdots + (\lambda_d - \mu_d) v_d
            = 0,
    \end{equation*}
    %
    so since the $v_i$ are affinely independent we have $\lambda_i = \mu_i$ by Proposition~2.9. Hence $f$ is well-defined.

    Next we show that $f$ is affine. Let
    %
    \begin{equation*}
        x = \lambda_0 v_0 + \cdots + \lambda_d v_d
        \quad \text{and} \quad
        y = \mu_0 v_0 + \cdots + \mu_d v_d
    \end{equation*}
    %
    with $\lambda_0 + \cdots + \lambda_d = 1$ and $\mu_0 + \cdots + \mu_d = 1$, and let $t \in \reals$. Then
    %
    \begin{equation*}
        \sum_{i=0}^d \bigl( (1-t)\lambda_i + t\mu_i \bigr)
            = (1-t) \sum_{i=0}^d\lambda_i + t \sum_{i=0}^d\mu_i
            = (1-t) + t
            = 1.
    \end{equation*}
    %
    Therefore, since
    %
    \begin{equation*}
        (1-t)x + ty
            = \sum_{i=0}^d \bigl( (1-t)\lambda_i + t\mu_i \bigr) v_i,
    \end{equation*}
    %
    we have
    %
    \begin{align*}
        f((1-t)x + ty)
            &= \bigl( (1-t)\lambda_0 + t\mu_0, \ldots, (1-t)\lambda_d + t\mu_d \bigr) \\
            &= (1-t)(\lambda_0, \ldots, \lambda_d) + t(\mu_0, \ldots, \mu_d) \\
            &= (1-t)f(x) + tf(y),
    \end{align*}
    %
    so $f$ is affine.
\end{solution}


\begin{exerciseframed*}[12]
    Prove that a non-empty open subset $U \subseteq \reals^d$ has dimension $\dim U = d$. Show that a subset $S \subseteq \reals^d$ with $\dim S = d$ contains a non-empty open subset.
\end{exerciseframed*}

\begin{solution}
    Let $x \in U$, and let $(e_1, \ldots, e_d)$ be the standard basis for $\reals^d$. Since $U$ is open there is some $\delta > 0$ such that $x + \delta e_i \in U$ for all $i$. The linear subspace $-x + \aff U$ has dimension $d$, so $U$ has dimension $d$.

    The latter claim is in fact false, since if $S$ is e.g. the collection of points $\delta e_i$ along with the origin, then $S$ has dimension $d$ (since $\aff S$ contains each of the coordinate axes, hence the entire $\reals^d$) but clearly contains no non-empty open subset.
\end{solution}


\chapter{Convex subsets}

\section*{Exercises}

\newcommand{\conv}{\operatorname{conv}}
\newcommand{\polar}[1]{{#1}^{\circ}}
\newcommand{\trans}{^{\top}}

\begin{exerciseframed*}[7]
    We state and prove a more general result that the one given in the exercise.

    For any subsets $A,B \subseteq \reals^d$ we have
    %
    \begin{equation*}
        \conv (A + B)
            = \conv A + \conv B.
    \end{equation*}
\end{exerciseframed*}

\begin{solution}
    The inclusion \enquote{$\subseteq$} is clear since the sum of convex sets is convex. For the converse direction, let $v \in \conv A$ and $w \in \conv B$, and write $v = \sum_{i=1}^n \lambda_i v_i$ and $w = \sum_{j=1}^m \mu_j w_j$ as convex combinations of elements in $A$ and $B$ respectively. Then
    %
    \begin{equation*}
        v + w_j
            = v + (\lambda_1 + \cdots + \lambda_n)w_j
            = \sum_{i=1}^n \lambda_i (v_i + w_j)
            \in \conv(A + B),
    \end{equation*}
    %
    and
    %
    \begin{equation*}
        v + w
            = (\mu_1 + \cdots + \mu_n)v + w
            = \sum_{j=1}^m \mu_j (v + w_j)
            \in \conv\conv(A + B)
            = \conv(A + B),
    \end{equation*}
    %
    as desired.
\end{solution}


\begin{exerciseframed*}[9]
    We state and prove a more general result that the one given in the exercise.

    For $S \subseteq \reals^d$, prove that $\polar{(\conv S)} = \polar{S}$. If $S = \{v_1, \ldots, v_n\}$ is finite, then
    %
    \begin{equation*}
        \polar{(\conv S)}
            = \set{\alpha \in \reals^d}{\alpha\trans v_1 \leq 1, \ldots, \alpha\trans v_n \leq 1}
            = \set{\alpha \in \reals^d}{A\alpha \leq 1},
    \end{equation*}
    %
    where $A$ is the $n \prod d$ matrix with rows $v_1\trans, \ldots, v_n\trans$. In particular, polytopes are polyhedra.
\end{exerciseframed*}

\begin{solution}
    Since $S \subseteq \conv S$, we clearly have $\polar{(\conv S)} \subseteq \polar{S}$. For the opposite inclusion, let $\alpha \in \polar{S}$ and let $\lambda_1 v_1 + \cdots + \lambda_n v_n$ be a convex combination of elements in $S$. Then
    %
    \begin{equation*}
        \alpha\trans(\lambda_1 v_1 + \cdots + \lambda_n v_n)
            = \lambda_1 \alpha\trans v_1 + \cdots + \lambda_n \alpha\trans v_n
            \leq \lambda_1 + \cdots \lambda_n
            = 1,
    \end{equation*}
    %
    so $\alpha \in \polar{(\conv S)}$. The rest of the claims follow easily.
\end{solution}


\begin{exerciseframed*}[10]
    If $F \subseteq G \subseteq C$ are convex subsets of $\reals^d$, prove that $F$ is a face of $C$ if $F$ is a face of $G$ and $G$ is a face of $C$.
\end{exerciseframed*}

\begin{solution}
    Let $x,y \in C$ such that the open line segment $(x,y)$ intersects $F$. Then it intersects $G$, so since $G$ is a face of $C$ we have $x,y \in G$. Similarly, since $F$ is a face of $G$ we then have $x,y \in F$. Hence $F$ is a face of $C$.
\end{solution}


\begin{exerciseframed*}[12]
    Prove that $C \setminus F$ is a convex subset if $F$ is a face of a convex subset $C$. Is it true that $F \subseteq C$ is a face if $C \setminus F$ is a convex subset?
\end{exerciseframed*}

\begin{solution}
    Let $x,y \in C \setminus F$. If the open line segment $(x,y)$ intersected $F$, then we would have $x,y \in F$ since $F$ is a face of $C$. Thus $(x,y) \subseteq C \setminus F$, so in total $[x,y] \subseteq C \setminus F$. Hence $C \setminus F$ is convex.

    The converse is false. For instance, remove (at least) two extreme points from a disk.
\end{solution}

\end{document}